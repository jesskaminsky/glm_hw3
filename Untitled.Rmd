---
title: "hw3"
author: "Jess Kaminsky"
date: "March 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
library(locfit)
library(ggplot2)
logit<-function(x) log(x/(1-x))
library(knitr)
```

## GELMAN HILL CHAPTER 5 - QUESTION 3

$$

\beta_0 = logit(0.27) = -0.9946\\

logit(.88) = -0.9946 + \beta_1 * 6\\
\beta_1 = 0.4978\\

logit(y) = -0.9946 + 0.4978x\\
y = logit^{-1}(-0.9946 + 0.4978x) = expit(-0.9946 + 0.4978x) = \frac{e^{-0.9946 + 0.4978x}}{1 + e^{-0.9946 + 0.4978x}}
$$

## GELMAN HILL CHAPTER 5 - QUESTION 5

(a)  *Graph the fitted model. Also on this graph put a scatterplot of hypothetical data consistent with the information given.*
```{r, echo = FALSE}
hyp_data <- rnorm(n = 50, mean = 60, sd = 15)

prob_data <-  q5a(hyp_data)

for(i in 1:length(prob_data)) {
  y_data[i] <- rbinom(1,1,prob_data[i])
}


q5a = function(x){expit(-24 + 0.4*x)}
plot(q5a(1:100), type='l', xlab = "Midterm Score", ylab = "Probability of Passing Course", main = "Fitted Model: Pr(pass) = expit(âˆ’24 + 0.4x)")
points(hyp_data, y_data)
```

(b)  *Suppose the midterm scores were transformed to have a mean of 0 and stan-dard deviation of 1. What would be the equation of the logistic regressionusing these transformed scores as a predictor?*

In logistic regression, when the x's are standardized, the intercept term disappears. The standardized coefficients can be found by multiplying the estimate of $\beta$ by the standard deviation of the x's for that coefficient. 
$$
Standardized \beta_0 = 0\\
Standardized \beta_1 = \beta_1 * \sigma_{x1} = 0.4 * 15 = 6\\
logit(y) = 6x\\
y = logit^{-1}(6x) = expit(6x) = \frac{e^{6x}}{1 + e^{6x}}
$$

(c) When comparing the exact model predicting probability of passing from midterm score to a model with random noise added - the difference in residual deviance between the two models is close to 0. We can conclude that when a pure noise predictor is added the residual deviance should not decrease.
```{r, echo = FALSE}
noise <- rnorm (100,0,1)

exact_x <- seq(1,100,1)
exact_y <- q5a(exact_x)

exact_model <- glm(exact_y ~ exact_x,family = binomial(logit))
noise_model <- glm(exact_y ~ exact_x + noise, family = binomial(logit))

kable(cbind(c("Exact Model Deviance: ", "Null Model Deviance: ", "Difference in Deviance: "), c(exact_model$deviance, noise_model$deviance, noise_model$deviance - exact_model$deviance)))

```


## GELMAN HILL CHAPTER 5 - QUESTION 8


(a)  *Build a logistic regression model to predict the presence of rodents (the variable rodent2 in the dataset) given indicators for the ethnic groups (race).Combine categories as appropriate. Discuss the estimated coefficients in the model.*

```{r}
rodents <- read.table("rodents.txt")

rodents$race <- as.factor(rodents$race)
rodents$rodent2 <- as.factor(rodents$rodent2)
```

Below is the distribution of the race groups represented in our data set. Groups 6 and 7 are much smaller than the others - group 6 has only 7 subjects and group 7 has 10. We will continue the analysis by combining groups 6 and 7. 

```{r, echo = FALSE}
hist(as.numeric(rodents$race), xlab = "Race Group", ylab = "Frequency", main = "Distribution of Race", breaks = c(0:7))
```

After Combining groups 6 + 7, the distribution of race looks like the following.
```{r, echo = FALSE}
for(i in 1:length(rodents$race)) {
  if(rodents$race[i] == 7) {
    rodents$race[i] = 6
  }
}

hist(as.numeric(rodents$race), xlab = "Race Group", ylab = "Frequency", main = "Distribution of Race - Combined Race 6 + 7", breaks = c(0:7))
```

In building a logistic regression model predicting the presence of rodents from the indicator variable for ethnic group, we obtain the following results. The predicted model is as follows with race 1 as the reference group.

$$
logit(p) = -2.1521 + 1.5361(race2) + 1.4492(race3) + 1.8671(race4) + 0.4004(race5) + 1.3636(race6)
$$
From the following summary of the logistic regression model, the coefficent for race5 is not significantly different from 0. The coefficient for race6 is significantly different from 0 at the $\alpha = 0.10$ level. All other predictors are statistically significant at the $\alpha = 0.05$ level.
```{r}
rodent_presence <- glm(rodent2 ~ race,family = binomial(logit), data = rodents)

summary(rodent_presence)
```


(b) For the second part of this problem use the variables in the dataset to develop a good logistic regression model. Make sure to carefully explain what your final model means and check its fit using diagnostics. Make an ROC plot and a calibration curve.
```{r, echo = FALSE}
rodent_remove <- na.omit(rodents)
full_rodent <- glm(rodent2 ~ ., family = binomial(logit), data = rodent_remove)

stepwise_rodent <- step(full_rodent)

summary(stepwise_rodent)
```

## QUESTION 4

The full dehydration outcome for the Dhaka study that we looked at in class is a three-level variable (none, some or severe). Build two proportional odds regression models for this three-level variable. In the first use the clinical signs only.


# CLINICAL SIGNS ONLY
```{r, echo = FALSE}
library(nnet)
library(VGAM)
library(ordinal)
dhaka <- read.csv("dhaka.csv")
attach(dhaka)

modela <- vglm(ordered(dehyd) ~ capref + extrem + eyes + genapp + heart + mucous + pulse + resp + tears + thirst + urine,family=cumulative, data = dhaka)

summary(modela)
```

# ADDITIONAL PREDICTORS
In the second, add in the additional predictors. Keep variables that you feel are important to prediction and to interpretation. Interpret your results clearly.
```{r}
#library(MASS)
#full_dhaka <- na.omit(dhaka)

dhaka$eyes <- as.factor(dhaka$eyes)
dhaka$genapp <- as.factor(dhaka$genapp)
dhaka$heart <- as.factor(dhaka$heart)
dhaka$mucous <- as.factor(dhaka$mucous)
dhaka$pulse <- as.factor(dhaka$pulse)
dhaka$resp <- as.factor(dhaka$resp)
dhaka$skin <- as.factor(dhaka$skin)
dhaka$tears <- as.factor(dhaka$tears)
dhaka$thirst <- as.factor(dhaka$thirst)
dhaka$urine <- as.factor(dhaka$urine)

modelc <- vglm(ordered(dehyd) ~ capref + extrem + eyes + genapp + heart + mucous + pulse + resp + tears + thirst + urine,family=cumulative, data = dhaka)

modelb <- vglm(ordered(dehyd) ~ .,family=cumulative, data = full_dhaka)

scale(modelb)
```